"""Newton-CG trust-region optimization."""
from __future__ import division, print_function, absolute_import

import math

import numpy as np
import scipy.linalg
from ._trustregion import _help_solve_subproblem, _minimize_trust_region

__all__ = ['fmin_dogleg', 'fmin_trust_ncg']


def fmin_trust_ncg(f, x0, fprime, fhess=None, fhessp=None, args=(),
        initial_trust_radius=1.0,
        max_trust_radius=1000.0,
        eta=0.15,
        gtol=1e-4,
        maxiter=None, full_output=False, disp=True,
        retall=False, callback=None):
    """
    Minimize a function using a Newton-CG trust-region algorithm.

    This algorithm requires function values, first derivatives,
    and either the Hessian or a way to compute the product
    of a Hessian and a vector.
    The Hessian is not required to be positive semidefinite,
    and it is not decomposed.

    Parameters
    ----------
    f : callable ``f(x, *args)``
        Objective function to be minimized.
    x0 : ndarray
        Initial guess.
    fprime : callable ``f'(x, *args)``
        Gradient of f.
    fhess : callable ``fhess(x, *args)``
        Function to compute the Hessian matrix of f.
    fhessp : callable ``fhessp(x, p, *args)``
        Function to compute the product of the Hessian of f with vector p.
    args : tuple, optional
        Extra arguments passed to f, fprime, and fhess
        (the same set of extra arguments is supplied to all of
        these functions).
    initial_trust_radius : float, optional
        initial trust radius
    max_trust_radius : float, optional
        never propose steps that are longer than this value
    eta : float, optional
        trust region related acceptance stringency for proposed steps
    gtol : float, optional
        Gradient norm must be less than `gtol`
        before successful termination.
    maxiter : int, optional
        Maximum number of iterations to perform.
    full_output : bool, optional
        If True, return the optional outputs.
    disp : bool, optional
        If True, print convergence message.
    retall : bool, optional
        If True, return a list of results at each iteration
    callback : callable, optional
        An optional user-supplied function which is called after
        each iteration.  Called as callback(xk), where xk is the
        current parameter vector.

    Returns
    -------
    xopt : ndarray
        Parameters which minimize f, i.e. ``f(xopt) == fopt``.
    fopt : float
        Value of the function at xopt, i.e. ``fopt = f(xopt)``.
    fcalls : int
        Number of function calls made.
    gcalls : int
        Number of gradient calls made.
    hcalls : int
        Number of hessian calls made.
    warnflag : int
        Warnings generated by the algorithm.
        1 : Maximum number of iterations exceeded.
        2 : A bad approximation causes failure to predict improvement.
        3 : Linear algebra error.
    allvecs : list
        Results at each iteration.  Only returned if retall is True.

    See also
    --------
    minimize: Interface to minimization algorithms for multivariate
        functions.  See the 'trust_ncg' `method` in particular.

    References
    ----------
    .. [1] Jorge Nocedal and Stephen Wright,
           Numerical Optimization, second edition,
           Springer-Verlag, 2006, page 171.

    """
    opts = {'initial_trust_radius': initial_trust_radius,
            'max_trust_radius': max_trust_radius,
            'eta': eta,
            'gtol': gtol,
            'disp': disp,
            'maxiter': maxiter,
            'return_all': retall}

    res = _minimize_trust_region(
            f, x0, args, jac=fprime, hess=fhess, hessp=fhessp,
            solve_subproblem=_solve_subproblem_cg_steihaug,
            callback=callback, **opts)

    if full_output:
        retlist = (res['x'], res['fun'], res['nfev'], res['njev'],
                res['nhev'], res['status'])
        if retall:
            retlist += (res['allvecs'], )
        return retlist
    else:
        if retall:
            return res['x'], res['allvecs']
        else:
            return res['x']


def _solve_subproblem_cg_steihaug(m, trust_radius, tolerance=None):
    """
    Solve the subproblem using a conjugate gradient method.

    Parameters
    ----------
    m : LazyLocalQuadraticModel
        The quadratic model of the objective function.
    trust_radius : float
        We are allowed to wander only this far away from the origin.
    tolerance : float, optional
        Solve the sub-problem with this much tolerance.

    Returns
    -------
    p : ndarray
        The proposed step.
    hits_boundary : bool
        True if the proposed step is on the boundary of the trust region.

    Notes
    -----
    This is algorithm (7.2) of Nocedal and Wright 2nd edition.
    Only the function that computes the Hessian-vector product is required.
    The Hessian itself is not required, and the Hessian does
    not need to be positive semidefinite.

    This function is called by the `_minimize_trust_region` function.
    It is not supposed to be called directly.
    """

    # get the norm of jacobian and define the origin
    jac_norm = scipy.linalg.norm(m.jac())
    p_origin = np.zeros_like(m.jac())

    # define a default tolerance
    if tolerance is None:
        tolerance = min(0.5, math.sqrt(jac_norm)) * jac_norm

    # Stop the method if the search direction
    # is a direction of nonpositive curvature.
    if jac_norm < tolerance:
        hits_boundary = False
        return p_origin, hits_boundary

    # init the state for the first iteration
    z = p_origin
    r = m.jac()
    d = -r

    # Search for the min of the approximation of the objective function.
    while True:

        # do an iteration
        Bd = m.hessp(d)
        dBd = np.dot(d, Bd)
        if dBd <= 0:
            # Look at the two boundary points.
            # Find both values of t to get the boundary points such that
            # ||z + t d|| == trust_radius
            # and then choose the one with the predicted min value.
            ta, tb = _help_solve_subproblem(z, d, trust_radius)
            pa = z + ta * d
            pb = z + tb * d
            if m(pa) < m(pb):
                p_boundary = pa
            else:
                p_boundary = pb
            hits_boundary = True
            return p_boundary, hits_boundary
        r_squared = np.dot(r, r)
        alpha = r_squared / dBd
        z_next = z + alpha * d
        if scipy.linalg.norm(z_next) >= trust_radius:
            # Find t >= 0 to get the boundary point such that
            # ||z + t d|| == trust_radius
            ta, tb = _help_solve_subproblem(z, d, trust_radius)
            p_boundary = z + tb * d
            hits_boundary = True
            return p_boundary, hits_boundary
        r_next = r + alpha * Bd
        r_next_squared = np.dot(r_next, r_next)
        if math.sqrt(r_next_squared) < tolerance:
            hits_boundary = False
            return z_next, hits_boundary
        beta_next = r_next_squared / r_squared
        d_next = -r_next + beta_next * d

        # update the state for the next iteration
        z = z_next
        r = r_next
        d = d_next

